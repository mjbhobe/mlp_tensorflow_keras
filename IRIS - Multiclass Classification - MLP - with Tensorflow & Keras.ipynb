{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>IRIS - Multiclass Classification with Tensorflow and Keras using a Multilayer Perceptron (MLP)</center>\n",
    "<center><img src=\"keras-tensorflow-logo.jpg\"></center>\n",
    "\n",
    "In this notebook, **we will illustrate multi-class classification of the IRIS dataset using a Multilayer Perceptron (MLP) (also called an ANN).** This dataset will be downloaded from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data)\n",
    "\n",
    "The IRIS dataset is a popular dataset, used in several examples with the `scikit-learn` library. The dataset had 150 records for 3 Iris flower species (Iris-setosa, Iris-versicolor and Iris-virginica). Each species is classified using 4 features - sepal length, sepal width, petal length and petal width. There are 50 records per species of flower. A complete description of the dataset is provided [here](https://archive.ics.uci.edu/ml/datasets/iris). \n",
    "\n",
    "**NOTE:** Strictly speaking, we need not use deep-learning techniques for classification of such simple, structured datasets. Traditional techniques (i.e. classifiers from the `scikit-learn` library) should suffice. However, using such a dataset serves as a good starter point for someone coming over to deep learning from a traditional machine learning background, where they have been using `scikit-learn` and `XGBoost` libraries.\n",
    "\n",
    "In this notebook we will first develop a MLP using Tensorflow. Thereafter the same network architecture will be developed using Keras (running with Tensorflow backend). I expect to see similar performance out of both networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# imports & tweaks\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys, time\n",
    "\n",
    "# tweaks for numpy & plotting libraries\n",
    "float_formatter = lambda x: '%.4f' % x\n",
    "np.set_printoptions(formatter={'float_kind':float_formatter})\n",
    "np.set_printoptions(threshold=np.inf, suppress=True, precision=4, linewidth=2048)\n",
    "\n",
    "plt.style.use(\"seaborn-colorblind\")\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 6)\n",
    "sns.set_style(\"darkgrid\")\n",
    "#sns.set_context(\"talk\")\n",
    "sns.set_context(context='notebook', font_scale=1.25)\n",
    "sns.set_style({\"font.sans-serif\": [\"Verdana\", \"Arial\", \"Calibri\", \"DejaVu Sans\"]})\n",
    "\n",
    "# NOTE: Always use a seed for random number generators, so you get same results across runs\n",
    "# you can use any number as the seed \n",
    "seed = 101\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading & Pre-processing the Images\n",
    "We will download the IRIS dataset from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data). We will use the Pandas library to pre-process the data. The downloaded dataset is in CSV format, sorted by the species columns - first 50 records are for Iris-setosa, next 50 for Iris-versicolor and last 50 for Iris-virginical species. The features are ordered (left to right) as `sepal length, sepal width, petal length & petal width`. The dataset does not have column headers - we will provide these too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# tweaks for pandas\n",
    "pd.set_option('display.notebook_repr_html',True)\n",
    "pd.set_option('display.max_rows', 15)\n",
    "pd.set_option('display.max_columns', 25)\n",
    "pd.set_option('display.width', 1024)\n",
    "pd.set_option('display.float_format', float_formatter)\n",
    "\n",
    "col_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
    "\n",
    "# helper function to download the data from UCI ML repository URL\n",
    "def load_data():\n",
    "    url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "    iris_df = pd.read_csv(url, header=None, names=col_names)\n",
    "    species = np.unique(iris_df['species'])\n",
    "    return iris_df, species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5.7000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>4.2000</td>\n",
       "      <td>1.2000</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.8000</td>\n",
       "      <td>3.4000</td>\n",
       "      <td>1.6000</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>5.5000</td>\n",
       "      <td>2.4000</td>\n",
       "      <td>3.7000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>5.9000</td>\n",
       "      <td>3.2000</td>\n",
       "      <td>4.8000</td>\n",
       "      <td>1.8000</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>6.1000</td>\n",
       "      <td>2.9000</td>\n",
       "      <td>4.7000</td>\n",
       "      <td>1.4000</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>6.3000</td>\n",
       "      <td>2.3000</td>\n",
       "      <td>4.4000</td>\n",
       "      <td>1.3000</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>6.6000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>4.4000</td>\n",
       "      <td>1.4000</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4000</td>\n",
       "      <td>3.9000</td>\n",
       "      <td>1.7000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>6.7000</td>\n",
       "      <td>3.1000</td>\n",
       "      <td>5.6000</td>\n",
       "      <td>2.4000</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>5.0000</td>\n",
       "      <td>3.5000</td>\n",
       "      <td>1.3000</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>5.0000</td>\n",
       "      <td>3.3000</td>\n",
       "      <td>1.4000</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>6.0000</td>\n",
       "      <td>2.7000</td>\n",
       "      <td>5.1000</td>\n",
       "      <td>1.6000</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>6.4000</td>\n",
       "      <td>2.7000</td>\n",
       "      <td>5.3000</td>\n",
       "      <td>1.9000</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>6.3000</td>\n",
       "      <td>3.4000</td>\n",
       "      <td>5.6000</td>\n",
       "      <td>2.4000</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width          species\n",
       "95         5.7000       3.0000        4.2000       1.2000  Iris-versicolor\n",
       "11         4.8000       3.4000        1.6000       0.2000      Iris-setosa\n",
       "81         5.5000       2.4000        3.7000       1.0000  Iris-versicolor\n",
       "70         5.9000       3.2000        4.8000       1.8000  Iris-versicolor\n",
       "63         6.1000       2.9000        4.7000       1.4000  Iris-versicolor\n",
       "87         6.3000       2.3000        4.4000       1.3000  Iris-versicolor\n",
       "75         6.6000       3.0000        4.4000       1.4000  Iris-versicolor\n",
       "..            ...          ...           ...          ...              ...\n",
       "5          5.4000       3.9000        1.7000       0.4000      Iris-setosa\n",
       "140        6.7000       3.1000        5.6000       2.4000   Iris-virginica\n",
       "40         5.0000       3.5000        1.3000       0.3000      Iris-setosa\n",
       "49         5.0000       3.3000        1.4000       0.2000      Iris-setosa\n",
       "83         6.0000       2.7000        5.1000       1.6000  Iris-versicolor\n",
       "111        6.4000       2.7000        5.3000       1.9000   Iris-virginica\n",
       "136        6.3000       3.4000        5.6000       2.4000   Iris-virginica\n",
       "\n",
       "[20 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df, species = load_data()\n",
    "# select 20 random records & display\n",
    "indexes = np.random.randint(0, len(iris_df), 20)\n",
    "#iris_df.head()  # display first 5\n",
    "iris_df.iloc[indexes,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Iris-setosa' 'Iris-versicolor' 'Iris-virginica']\n"
     ]
    }
   ],
   "source": [
    "print(species)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following pre-processing steps will be applied:\n",
    "* The `species` column is categorical (text) - we will encode/map to numeric values using following mapping `{'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2}`\n",
    "* The data is sorted by `species` columns (i.e. first 50 rows are for Iris-setosa, next 50 for Iris-versicolor and last 50 for Iris-virginica) - we will shuffle the data so that train & test sets are randomly picked\n",
    "* The features (sepal_length, sepal_width, petal_length & petal_width) will be standard scaled & labels (species) will be one-hot encoded to 3 columns (corresponding to 3 output classes/species)\n",
    "These transformations are applied across several of the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2}\n",
      "{0: 'Iris-setosa', 1: 'Iris-versicolor', 2: 'Iris-virginica'}\n",
      "encoded_array -> [0 1 2]\n",
      "decoded_array -> ['Iris-setosa' 'Iris-versicolor' 'Iris-virginica']\n"
     ]
    }
   ],
   "source": [
    "# helper functions to encode/decode columns\n",
    "labels2code_map = {j:i for i, j in enumerate(species)}\n",
    "code2labels_map = {i:j for i, j in enumerate(species)}\n",
    "print(labels2code_map)\n",
    "print(code2labels_map)\n",
    "\n",
    "def encode_array(arr):\n",
    "    # convert labels to code\n",
    "    encoder = np.vectorize(lambda label : labels2code_map[label])\n",
    "    return encoder(arr)\n",
    "    \n",
    "def decode_array(arr):\n",
    "    # convert code to labels\n",
    "    decoder = np.vectorize(lambda code : code2labels_map[code])\n",
    "    return decoder(arr)\n",
    "\n",
    "encoded_array = encode_array(species)\n",
    "print('encoded_array -> {}'.format(encoded_array))\n",
    "print('decoded_array -> {}'.format(decode_array(encoded_array)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>6.4000</td>\n",
       "      <td>3.2000</td>\n",
       "      <td>5.3000</td>\n",
       "      <td>2.3000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>6.4000</td>\n",
       "      <td>3.2000</td>\n",
       "      <td>5.3000</td>\n",
       "      <td>2.3000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>5.2000</td>\n",
       "      <td>2.7000</td>\n",
       "      <td>3.9000</td>\n",
       "      <td>1.4000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>5.1000</td>\n",
       "      <td>3.8000</td>\n",
       "      <td>1.9000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>6.3000</td>\n",
       "      <td>2.5000</td>\n",
       "      <td>4.9000</td>\n",
       "      <td>1.5000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.1000</td>\n",
       "      <td>3.8000</td>\n",
       "      <td>1.5000</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>6.2000</td>\n",
       "      <td>2.8000</td>\n",
       "      <td>4.8000</td>\n",
       "      <td>1.8000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4000</td>\n",
       "      <td>2.9000</td>\n",
       "      <td>1.4000</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>6.0000</td>\n",
       "      <td>2.2000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5.7000</td>\n",
       "      <td>2.8000</td>\n",
       "      <td>4.1000</td>\n",
       "      <td>1.3000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.2000</td>\n",
       "      <td>3.4000</td>\n",
       "      <td>1.4000</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>6.1000</td>\n",
       "      <td>2.9000</td>\n",
       "      <td>4.7000</td>\n",
       "      <td>1.4000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0000</td>\n",
       "      <td>3.4000</td>\n",
       "      <td>1.5000</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>6.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>4.8000</td>\n",
       "      <td>1.8000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width  species\n",
       "115        6.4000       3.2000        5.3000       2.3000        2\n",
       "115        6.4000       3.2000        5.3000       2.3000        2\n",
       "59         5.2000       2.7000        3.9000       1.4000        1\n",
       "44         5.1000       3.8000        1.9000       0.4000        0\n",
       "72         6.3000       2.5000        4.9000       1.5000        1\n",
       "19         5.1000       3.8000        1.5000       0.3000        0\n",
       "126        6.2000       2.8000        4.8000       1.8000        2\n",
       "..            ...          ...           ...          ...      ...\n",
       "8          4.4000       2.9000        1.4000       0.2000        0\n",
       "62         6.0000       2.2000        4.0000       1.0000        1\n",
       "99         5.7000       2.8000        4.1000       1.3000        1\n",
       "28         5.2000       3.4000        1.4000       0.2000        0\n",
       "63         6.1000       2.9000        4.7000       1.4000        1\n",
       "7          5.0000       3.4000        1.5000       0.2000        0\n",
       "138        6.0000       3.0000        4.8000       1.8000        2\n",
       "\n",
       "[20 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode the 'species' column of the dataframe\n",
    "iris_df['species'] = encode_array(iris_df['species'])\n",
    "# select 20 random records & display\n",
    "indexes = np.random.randint(0, len(iris_df), 20)\n",
    "#iris_df.head()  # display first 5\n",
    "iris_df.iloc[indexes,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features -> [[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]]\n",
      "Labels -> [0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# separate the data into features & labels\n",
    "features = iris_df.drop(['species'], axis=1).values\n",
    "labels = iris_df['species'].values\n",
    "print('Features -> {}'.format(features[:10]))\n",
    "print('Labels -> {}'.format(labels[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features -> [[4.6 3.4 1.4 0.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [5.5 3.5 1.3 0.2]]\n",
      "Labels -> [0 2 0 0 0 0 1 2 1 0]\n"
     ]
    }
   ],
   "source": [
    "# let's shuffle the features & labels\n",
    "indexes = np.arange(len(features))\n",
    "for _ in range(5): indexes = np.random.permutation(indexes)\n",
    "features = features[indexes,:]\n",
    "labels = labels[indexes]\n",
    "# display shuffled\n",
    "print('Features -> {}'.format(features[:10]))\n",
    "print('Labels -> {}'.format(labels[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4) (30, 4) (120,) (30,)\n"
     ]
    }
   ],
   "source": [
    "# split into train/test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(features, labels, test_size=0.2, random_state=seed)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4) (30, 4) (120, 3) (30, 3)\n"
     ]
    }
   ],
   "source": [
    "# standard scale features & one-hot encode labels\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have completed the preprocessing of the IRIS dataset - sweet!\n",
    "\n",
    "### Global Variables\n",
    "Next, let's define some global variables, which will be used across the Tensorflow & Keras models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_features = 4, num_classes = 3, epochs = 100, batch_size = 16\n",
      "Network layout = [4, 64, 64, 3]\n"
     ]
    }
   ],
   "source": [
    "num_features, num_classes, epochs, batch_size = X_train.shape[1], y_train.shape[1], 100, 16\n",
    "print('num_features = {}, num_classes = {}, epochs = {}, batch_size = {}'.format(\n",
    "            num_features, num_classes, epochs, batch_size))\n",
    "\n",
    "# layout of our MLP - 2 hidden layers with 64 nodes each\n",
    "layer_dims = [num_features, 64, 64, num_classes]\n",
    "print('Network layout = {}'.format(layer_dims))\n",
    "\n",
    "# for loading & saving model\n",
    "MODEL_SAVE_DIR, TF_MODEL_NAME, KR_MODEL_NAME = './model_states', 'tf_IRIS_mlp', 'kr_IRIS_mlp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Implementation\n",
    "In this section, we will develop the MLP using Tensorflow. We have developed several utility functions to help with model development - training, evaluation, loading & saving model state, predictions. These functions are available in the Python code file `tf_nn_helper.py`, which is imported into this workbook.\n",
    "\n",
    "### Building the Tensorflow Model\n",
    "Our MLP is a 4 layer network (including the input & output layers) - we have 2 hidden layers with 64 nodes each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.set_random_seed(seed)\n",
    "import tf_nn_helper as tfu # Tensorflow utility functions for creating/training/evaluating/loading & saving TF models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims, debug=False):\n",
    "    \"\"\" build the weights & biases for the network based on layout in LAYER_DIMS \"\"\"\n",
    "    weights, biases = {}, {}\n",
    "    L = len(layer_dims)\n",
    "    for l in range(1, L):\n",
    "        weights['W' + str(l)] = tf.get_variable('W' + str(l),\n",
    "                                                dtype=tf.float32,\n",
    "                                                shape=(layer_dims[l-1], layer_dims[l]),\n",
    "                                                initializer=tf.contrib.layers.xavier_initializer())\n",
    "        biases['b' + str(l)] = tf.get_variable('b' + str(l),\n",
    "                                                dtype=tf.float32,\n",
    "                                                shape=(1, layer_dims[l]),\n",
    "                                                initializer=tf.zeros_initializer())  \n",
    "        \n",
    "    if debug:\n",
    "        # display weights & biases\n",
    "        for i, w in enumerate(weights):\n",
    "            print('weights[W{}] = {}'.format(i, weights['W' + str(i+1)]))\n",
    "            print('biases[b{}] = {}'.format(i, biases['b' + str(i+1)]), flush=True)\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tf_model(X, y, kp, layer_dims, debug=False):\n",
    "    \n",
    "    def build_tf_nn(A0, weights, biases):\n",
    "        \"\"\" builds the network layers \"\"\"\n",
    "        A_prev = A0\n",
    "        L = len(weights)\n",
    "        for l in range(1, L):\n",
    "            W, b = weights['W' + str(l)], biases['b' + str(l)]\n",
    "            A = tfu.dense(A_prev, W, b, activation='relu', batch_normalize=False)\n",
    "            A = tf.nn.dropout(A, keep_prob=kp)\n",
    "            A_prev = A\n",
    "        # output layer - this will have softmax activation for multi-class classification\n",
    "        W, b = weights['W' + str(L)], biases['b' + str(L)]\n",
    "        out = tfu.dense(A_prev, W, b, activation='softmax', batch_normalize=False)\n",
    "        out = tfu.add_name_to_tensor(out, name='out')\n",
    "        return out\n",
    "    \n",
    "    weights, biases = initialize_parameters(layer_dims, debug=debug)\n",
    "    # our network tensors\n",
    "    out = build_tf_nn(X, weights, biases)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=out), name='loss')\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "    train_op = optimizer.minimize(loss, name='train_op')\n",
    "    correct_prediction = tf.equal(tf.argmax(out,axis=1), tf.argmax(y, axis=1))\n",
    "    tfu.add_name_to_tensor(correct_prediction, 'correct_prediction')\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='acc')\n",
    "    \n",
    "  #  now collect all these tensors & operators into a dict, which we can conveniently pass around\n",
    "  #  to the various utility functions we have created above\n",
    "  #  NOTE: keys of the dicts must be names EXACTLY as shown below, else the helper functions won't work.\n",
    "    model = {}\n",
    "    #model['weights'] = weights\n",
    "    #model['biases'] = biases\n",
    "    model['out'] = out\n",
    "    model['loss'] = loss\n",
    "    #model['optimizer'] = optimizer\n",
    "    model['train_op'] = train_op\n",
    "    model['correct_prediction'] = correct_prediction\n",
    "    model['accuracy'] = accuracy\n",
    "    return model      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tf_model(sess, base_file_name='network', model_path='./model_states'):\n",
    "    # following call will raise exception if it fails!\n",
    "    tf_graph = tfu.load_tf_model(sess, base_file_name, model_path)\n",
    "    \n",
    "    # restore elements from graph\n",
    "    X = tf_graph.get_tensor_by_name(\"X:0\")\n",
    "    y = tf_graph.get_tensor_by_name(\"y:0\")\n",
    "    kp = tf_graph.get_tensor_by_name(\"keep_prob:0\")\n",
    "\n",
    "    # restore model dict\n",
    "    model = {}\n",
    "    model['out'] = tf_graph.get_tensor_by_name(\"out:0\")\n",
    "    model['loss'] = tf_graph.get_tensor_by_name(\"loss:0\")\n",
    "    #model['optimizer'] = optimizer\n",
    "    model['train_op'] = tf_graph.get_operation_by_name(\"train_op\")\n",
    "    model['correct_prediction'] = tf_graph.get_tensor_by_name(\"correct_prediction:0\")\n",
    "    model['accuracy'] = tf_graph.get_tensor_by_name(\"acc:0\")\n",
    "    return model, X, y, kp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training our Tensorflow Model\n",
    "We will train our Tensorflow model through 50 epochs, using a batch size of 128. But we first need to create placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(debug=False):\n",
    "    # NOTE: tf_nn_helper expects 2 place holders with name \"X\" and \"y\"\n",
    "    # which is used to pass data to our model during training & evaluation\n",
    "    X = tf.placeholder(dtype=tf.float32, name=\"X\", shape=(None, num_features))\n",
    "    y = tf.placeholder(dtype=tf.float32, name=\"y\", shape=(None, num_classes))\n",
    "    kp = tf.placeholder(dtype=tf.float32, name='keep_prob')\n",
    "    if debug: print('Placeholders: X = {}, y = {}, kp={}'.format(X, y, kp), flush=True)\n",
    "    return X, y, kp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placeholders: X = Tensor(\"X:0\", shape=(?, 4), dtype=float32), y = Tensor(\"y:0\", shape=(?, 3), dtype=float32), kp=Tensor(\"keep_prob:0\", dtype=float32)\n",
      "{'out': <tf.Tensor 'out:0' shape=(?, 3) dtype=float32>, 'loss': <tf.Tensor 'loss:0' shape=() dtype=float32>, 'train_op': <tf.Operation 'train_op' type=NoOp>, 'correct_prediction': <tf.Tensor 'Equal:0' shape=(?,) dtype=bool>, 'accuracy': <tf.Tensor 'acc:0' shape=() dtype=float32>}\n"
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "tf.reset_default_graph()\n",
    "X, y, kp = create_placeholders(debug=True)\n",
    "tf_model = build_tf_model(X, y, kp, layer_dims)\n",
    "print(tf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120 samples. Train for 100 epochs,, with 8 batches per epoch.\n",
      "Epoch 1/100:\n",
      "   Batch (8/8) [==============================] -> loss: 1.0515 - acc: 0.5547                                        \n",
      "Epoch 2/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.9708 - acc: 0.6641                                        \n",
      "Epoch 3/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.9079 - acc: 0.7578                                        \n",
      "Epoch 4/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.8634 - acc: 0.7734                                        \n",
      "Epoch 5/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.8437 - acc: 0.7656                                        \n",
      "Epoch 6/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.8053 - acc: 0.8125                                        \n",
      "Epoch 7/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.7884 - acc: 0.8359                                        \n",
      "Epoch 8/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.7855 - acc: 0.8125                                        \n",
      "Epoch 9/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.7621 - acc: 0.8203                                        \n",
      "Epoch 10/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.7548 - acc: 0.8594                                        \n",
      "Epoch 11/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.7549 - acc: 0.8516                                        \n",
      "Epoch 12/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.7370 - acc: 0.8359                                        \n",
      "Epoch 13/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.7208 - acc: 0.8750                                        \n",
      "Epoch 14/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.7178 - acc: 0.8438                                        \n",
      "Epoch 15/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.7045 - acc: 0.8906                                        \n",
      "Epoch 16/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.7065 - acc: 0.8750                                        \n",
      "Epoch 17/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.7027 - acc: 0.8750                                        \n",
      "Epoch 18/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6941 - acc: 0.8516                                        \n",
      "Epoch 19/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.7080 - acc: 0.8359                                        \n",
      "Epoch 20/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6799 - acc: 0.9062                                        \n",
      "Epoch 21/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6806 - acc: 0.8906                                        \n",
      "Epoch 22/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6603 - acc: 0.9297                                        \n",
      "Epoch 23/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6705 - acc: 0.9141                                        \n",
      "Epoch 24/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6900 - acc: 0.8516                                        \n",
      "Epoch 25/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6607 - acc: 0.8828                                        \n",
      "Epoch 26/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6599 - acc: 0.9219                                        \n",
      "Epoch 27/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6522 - acc: 0.9141                                        \n",
      "Epoch 28/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6560 - acc: 0.9062                                        \n",
      "Epoch 29/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6641 - acc: 0.9141                                        \n",
      "Epoch 30/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6528 - acc: 0.9141                                        \n",
      "Epoch 31/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6502 - acc: 0.9141                                        \n",
      "Epoch 32/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6447 - acc: 0.9453                                        \n",
      "Epoch 33/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6404 - acc: 0.9375                                        \n",
      "Epoch 34/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6479 - acc: 0.9141                                        \n",
      "Epoch 35/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6449 - acc: 0.9297                                        \n",
      "Epoch 36/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6310 - acc: 0.9297                                        \n",
      "Epoch 37/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6325 - acc: 0.9453                                        \n",
      "Epoch 38/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6427 - acc: 0.9297                                        \n",
      "Epoch 39/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6447 - acc: 0.9141                                        \n",
      "Epoch 40/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6356 - acc: 0.9375                                        \n",
      "Epoch 41/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6285 - acc: 0.9531                                        \n",
      "Epoch 42/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6161 - acc: 0.9531                                        \n",
      "Epoch 43/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6359 - acc: 0.9219                                        \n",
      "Epoch 44/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6317 - acc: 0.9297                                        \n",
      "Epoch 45/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6171 - acc: 0.9609                                        \n",
      "Epoch 46/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6276 - acc: 0.9453                                        \n",
      "Epoch 47/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6086 - acc: 0.9531                                        \n",
      "Epoch 48/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6059 - acc: 0.9688                                        \n",
      "Epoch 49/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6108 - acc: 0.9453                                        \n",
      "Epoch 50/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6212 - acc: 0.9453                                        \n",
      "Epoch 51/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6186 - acc: 0.9453                                        \n",
      "Epoch 52/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6095 - acc: 0.9531                                        \n",
      "Epoch 53/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6056 - acc: 0.9531                                        \n",
      "Epoch 54/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6103 - acc: 0.9688                                        \n",
      "Epoch 55/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6098 - acc: 0.9609                                        \n",
      "Epoch 56/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6027 - acc: 0.9609                                        \n",
      "Epoch 57/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6065 - acc: 0.9531                                        \n",
      "Epoch 58/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6127 - acc: 0.9375                                        \n",
      "Epoch 59/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6131 - acc: 0.9453                                        \n",
      "Epoch 60/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6097 - acc: 0.9531                                        \n",
      "Epoch 61/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6020 - acc: 0.9688                                        \n",
      "Epoch 62/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6059 - acc: 0.9531                                        \n",
      "Epoch 63/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6041 - acc: 0.9609                                        \n",
      "Epoch 64/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6031 - acc: 0.9531                                        \n",
      "Epoch 65/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.5932 - acc: 0.9688                                        \n",
      "Epoch 66/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.5964 - acc: 0.9609                                        \n",
      "Epoch 67/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.5933 - acc: 0.9766                                        \n",
      "Epoch 68/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6045 - acc: 0.9609                                        \n",
      "Epoch 69/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.5992 - acc: 0.9531                                        \n",
      "Epoch 70/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.5888 - acc: 0.9688                                        \n",
      "Epoch 71/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.5923 - acc: 0.9688                                        \n",
      "Epoch 72/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.5876 - acc: 0.9766                                        \n",
      "Epoch 73/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6014 - acc: 0.9453                                        \n",
      "Epoch 74/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.5997 - acc: 0.9531                                        \n",
      "Epoch 75/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6043 - acc: 0.9609                                        \n",
      "Epoch 76/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.5828 - acc: 0.9844                                        \n",
      "Epoch 77/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.5933 - acc: 0.9688                                        \n",
      "Epoch 78/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.5795 - acc: 0.9922                                        \n",
      "Epoch 79/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.5886 - acc: 0.9688                                        \n",
      "Epoch 80/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.5925 - acc: 0.9688                                        \n",
      "Epoch 81/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.5858 - acc: 0.9688                                        \n",
      "Epoch 82/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6011 - acc: 0.9531                                        \n",
      "Epoch 83/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.5892 - acc: 0.9766                                        \n",
      "Epoch 84/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6022 - acc: 0.9531                                        \n",
      "Epoch 85/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.5901 - acc: 0.9688                                        \n",
      "Epoch 86/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.6017 - acc: 0.9531                                        \n",
      "Epoch 87/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.5979 - acc: 0.9609                                        \n",
      "Epoch 88/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.5816 - acc: 0.9766                                        \n",
      "Epoch 89/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.5948 - acc: 0.9531                                        \n",
      "Epoch 90/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.5924 - acc: 0.9609                                        \n",
      "Epoch 91/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.5855 - acc: 0.9688                                        \n",
      "Epoch 92/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.5950 - acc: 0.9531                                        \n",
      "Epoch 93/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.5996 - acc: 0.9453                                        \n",
      "Epoch 94/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.5995 - acc: 0.9453                                        \n",
      "Epoch 95/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.5789 - acc: 0.9766                                        \n",
      "Epoch 96/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.5882 - acc: 0.9766                                        \n",
      "Epoch 97/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.5995 - acc: 0.9609                                        \n",
      "Epoch 98/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.5786 - acc: 0.9844                                        \n",
      "Epoch 99/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.5983 - acc: 0.9609                                        \n",
      "Epoch 100/100:\n",
      "   Batch (8/8) [==============================] -> loss: 0.5904 - acc: 0.9688                                        \n",
      "Training completed in Time taken: 4 secs\n"
     ]
    }
   ],
   "source": [
    "# train our model\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# def train_model(sess, model, data, labels, feed_dict=None, num_epochs=10, batch_size=32,\n",
    "#                 validation_split=None, validation_data=None)\n",
    "start_time = time.time()\n",
    "feed_dict_train = {kp:(1.0-0.35)} # just pass in the keep probability placeholders\n",
    "history = tfu.train_model(sess, tf_model, data=X_train, labels=y_train, feed_dict=feed_dict_train, \n",
    "                          num_epochs=epochs, batch_size=batch_size)\n",
    "end_time = time.time()\n",
    "print('Training completed in {}'.format(tfu.time_taken_as_str(start_time, end_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tfu.show_plots(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating performance of Tensorflow model\n",
    "Let's evaluate performance of our model on training, cross-validation & test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating performance:\n",
      "  Evaluating : Batch (8/8) [==============================] -> loss: 0.5741 - acc: 0.9844                                        \n",
      "    - Training dataset: loss = 0.5741, accuracy = 0.9844\n",
      "  Evaluating : Batch (2/2) [==============================] -> loss: 0.5816 - acc: 0.9688                                        \n",
      "    - Test dataset: loss = 0.5816, accuracy = 0.9688\n"
     ]
    }
   ],
   "source": [
    "print('Evaluating performance:')\n",
    "feed_dict_eval = {kp:(1.0)} # NO droput during evaluation!!\n",
    "loss, acc = tfu.evaluate_model(sess, tf_model, X_train, y_train, feed_dict=feed_dict_eval, batch_size=batch_size)\n",
    "print('    - Training dataset: loss = {:.4f}, accuracy = {:.4f}'.format(loss, acc))\n",
    "\n",
    "loss, acc = tfu.evaluate_model(sess, tf_model, X_test, y_test, feed_dict=feed_dict_eval, batch_size=batch_size)\n",
    "print('    - Test dataset: loss = {:.4f}, accuracy = {:.4f}'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations for Tensorflow Model:**\n",
    "* We get a `training accuracy = 98.4%` and a `test accuracy = ~97%` with this model.\n",
    "* Since the difference between training & cross-validation accuracy is not much, we can conclude the thet model is not overfitting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model state & close session\n",
    "tfu.save_tf_model(sess, base_file_name=TF_MODEL_NAME, model_path=MODEL_SAVE_DIR)\n",
    "del tf_model\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions with the Tensorflow Model\n",
    "Next we will run predictions with our Tensorflow model & view some random results. \n",
    "\n",
    "NOTE: Model structure (graph) & state (parameters) will be loaded from last saved state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model_states/tf_IRIS_mlp\n",
      "Loaded model\n",
      "{'out': <tf.Tensor 'out:0' shape=(?, 3) dtype=float32>, 'loss': <tf.Tensor 'loss:0' shape=() dtype=float32>, 'train_op': <tf.Operation 'train_op' type=NoOp>, 'correct_prediction': <tf.Tensor 'correct_prediction:0' shape=(?,) dtype=bool>, 'accuracy': <tf.Tensor 'acc:0' shape=() dtype=float32>}\n",
      "Loaded placeholders: X = Tensor(\"X:0\", shape=(?, 4), dtype=float32), y = Tensor(\"y:0\", shape=(?, 3), dtype=float32), kp=Tensor(\"keep_prob:0\", dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "tf_model, X, y, kp = load_tf_model(sess, base_file_name=TF_MODEL_NAME, model_path=MODEL_SAVE_DIR)\n",
    "print('Loaded model')\n",
    "print(tf_model)\n",
    "print('Loaded placeholders: X = {}, y = {}, kp={}'.format(X, y, kp), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 0, 1, 2, 0, 1, 0, 2, 1, 1, 2, 1, 1, 1, 2, 1, 0, 2, 1, 1, 1, 0, 0, 1, 0, 2, 2, 0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the ground truth (reverse one-hot encode of test data)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "y_true[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 0, 1, 2, 0, 1, 0, 2, 1, 1, 2, 1, 2, 1, 2, 1, 0, 2, 1, 1, 1, 0, 0, 1, 0, 2, 2, 0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_dict_eval = {kp:(1.0)} # NO droput during evaluation!!\n",
    "predictions = tfu.predict(sess, tf_model, X_test, feed_dict=feed_dict_eval) # no dropouts when running predictions\n",
    "# reverse one-hot encode the predictions\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "y_pred[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many mismatches\n",
    "(y_pred != y_true).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close the Tensorflow session\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Implementation\n",
    "In this section, we will develop the same model using the Keras API. You will notice immediately that the code written in this section is much less & much cleaner. Keras takes care of most of the details we covered above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import optimizers\n",
    "import kr_helper_funcs as kru\n",
    "\n",
    "# clear the Tensorflow backend to get rid of any spurious graphs\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_keras_model(layer_dims):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(layer_dims[1], activation='relu', input_shape=(layer_dims[0],)))\n",
    "    model.add(Dropout(0.35))\n",
    "    model.add(Dense(layer_dims[2], activation='relu'))\n",
    "    model.add(Dropout(0.35))\n",
    "    # output is softmax\n",
    "    model.add(Dense(layer_dims[3], activation='softmax'))\n",
    "    adam = optimizers.Adam(lr=0.001)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                320       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 4,675\n",
      "Trainable params: 4,675\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "kr_model = build_keras_model(layer_dims)\n",
    "print(kr_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 1.0810 - acc: 0.3750\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 0s 244us/step - loss: 0.9144 - acc: 0.6000\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 0s 221us/step - loss: 0.7761 - acc: 0.7667\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 0s 230us/step - loss: 0.6782 - acc: 0.7833\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 0s 225us/step - loss: 0.6057 - acc: 0.8167\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 0s 188us/step - loss: 0.5528 - acc: 0.7833\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.4893 - acc: 0.8083\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.4477 - acc: 0.8250\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.4118 - acc: 0.8250\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.3852 - acc: 0.8250\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 0s 263us/step - loss: 0.3729 - acc: 0.8333\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 0s 208us/step - loss: 0.3492 - acc: 0.8583\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 0s 249us/step - loss: 0.3410 - acc: 0.8500\n",
      "Epoch 14/100\n",
      "120/120 [==============================] - 0s 221us/step - loss: 0.3092 - acc: 0.8333\n",
      "Epoch 15/100\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.2956 - acc: 0.8833\n",
      "Epoch 16/100\n",
      "120/120 [==============================] - 0s 179us/step - loss: 0.3458 - acc: 0.8417\n",
      "Epoch 17/100\n",
      "120/120 [==============================] - 0s 195us/step - loss: 0.2909 - acc: 0.8750\n",
      "Epoch 18/100\n",
      "120/120 [==============================] - 0s 211us/step - loss: 0.3323 - acc: 0.8167\n",
      "Epoch 19/100\n",
      "120/120 [==============================] - 0s 228us/step - loss: 0.2982 - acc: 0.8750\n",
      "Epoch 20/100\n",
      "120/120 [==============================] - 0s 202us/step - loss: 0.2616 - acc: 0.9083\n",
      "Epoch 21/100\n",
      "120/120 [==============================] - 0s 177us/step - loss: 0.2717 - acc: 0.8667\n",
      "Epoch 22/100\n",
      "120/120 [==============================] - 0s 244us/step - loss: 0.2764 - acc: 0.8667\n",
      "Epoch 23/100\n",
      "120/120 [==============================] - 0s 220us/step - loss: 0.2580 - acc: 0.9167\n",
      "Epoch 24/100\n",
      "120/120 [==============================] - 0s 200us/step - loss: 0.2462 - acc: 0.9083\n",
      "Epoch 25/100\n",
      "120/120 [==============================] - 0s 250us/step - loss: 0.2499 - acc: 0.8833\n",
      "Epoch 26/100\n",
      "120/120 [==============================] - 0s 200us/step - loss: 0.2568 - acc: 0.9083\n",
      "Epoch 27/100\n",
      "120/120 [==============================] - 0s 196us/step - loss: 0.2340 - acc: 0.9250\n",
      "Epoch 28/100\n",
      "120/120 [==============================] - 0s 174us/step - loss: 0.2124 - acc: 0.8917\n",
      "Epoch 29/100\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.2336 - acc: 0.9500\n",
      "Epoch 30/100\n",
      "120/120 [==============================] - 0s 240us/step - loss: 0.2370 - acc: 0.9250\n",
      "Epoch 31/100\n",
      "120/120 [==============================] - 0s 246us/step - loss: 0.2412 - acc: 0.9000\n",
      "Epoch 32/100\n",
      "120/120 [==============================] - 0s 191us/step - loss: 0.2279 - acc: 0.8833\n",
      "Epoch 33/100\n",
      "120/120 [==============================] - 0s 191us/step - loss: 0.2493 - acc: 0.9083\n",
      "Epoch 34/100\n",
      "120/120 [==============================] - 0s 246us/step - loss: 0.2049 - acc: 0.9167\n",
      "Epoch 35/100\n",
      "120/120 [==============================] - 0s 181us/step - loss: 0.1756 - acc: 0.9500\n",
      "Epoch 36/100\n",
      "120/120 [==============================] - 0s 270us/step - loss: 0.2169 - acc: 0.9167\n",
      "Epoch 37/100\n",
      "120/120 [==============================] - 0s 178us/step - loss: 0.1462 - acc: 0.9667\n",
      "Epoch 38/100\n",
      "120/120 [==============================] - 0s 213us/step - loss: 0.1887 - acc: 0.9250\n",
      "Epoch 39/100\n",
      "120/120 [==============================] - 0s 210us/step - loss: 0.1668 - acc: 0.9333\n",
      "Epoch 40/100\n",
      "120/120 [==============================] - 0s 240us/step - loss: 0.1427 - acc: 0.9583\n",
      "Epoch 41/100\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.1643 - acc: 0.9583\n",
      "Epoch 42/100\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.1620 - acc: 0.9250\n",
      "Epoch 43/100\n",
      "120/120 [==============================] - 0s 197us/step - loss: 0.1478 - acc: 0.9333\n",
      "Epoch 44/100\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.1575 - acc: 0.9333\n",
      "Epoch 45/100\n",
      "120/120 [==============================] - 0s 235us/step - loss: 0.1497 - acc: 0.9333\n",
      "Epoch 46/100\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.1356 - acc: 0.9583\n",
      "Epoch 47/100\n",
      "120/120 [==============================] - 0s 176us/step - loss: 0.1220 - acc: 0.9667\n",
      "Epoch 48/100\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.1139 - acc: 0.9500\n",
      "Epoch 49/100\n",
      "120/120 [==============================] - 0s 224us/step - loss: 0.1350 - acc: 0.9500\n",
      "Epoch 50/100\n",
      "120/120 [==============================] - 0s 203us/step - loss: 0.1638 - acc: 0.9417\n",
      "Epoch 51/100\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.1218 - acc: 0.9667\n",
      "Epoch 52/100\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.0969 - acc: 0.9750\n",
      "Epoch 53/100\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.1239 - acc: 0.9500\n",
      "Epoch 54/100\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.1306 - acc: 0.9500\n",
      "Epoch 55/100\n",
      "120/120 [==============================] - 0s 208us/step - loss: 0.1124 - acc: 0.9833\n",
      "Epoch 56/100\n",
      "120/120 [==============================] - 0s 221us/step - loss: 0.0877 - acc: 0.9667\n",
      "Epoch 57/100\n",
      "120/120 [==============================] - 0s 195us/step - loss: 0.1353 - acc: 0.9583\n",
      "Epoch 58/100\n",
      "120/120 [==============================] - 0s 196us/step - loss: 0.1541 - acc: 0.9333\n",
      "Epoch 59/100\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.1296 - acc: 0.9667\n",
      "Epoch 60/100\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.1104 - acc: 0.9500\n",
      "Epoch 61/100\n",
      "120/120 [==============================] - 0s 199us/step - loss: 0.1210 - acc: 0.9583\n",
      "Epoch 62/100\n",
      "120/120 [==============================] - 0s 197us/step - loss: 0.1379 - acc: 0.9667\n",
      "Epoch 63/100\n",
      "120/120 [==============================] - 0s 221us/step - loss: 0.1255 - acc: 0.9500\n",
      "Epoch 64/100\n",
      "120/120 [==============================] - 0s 197us/step - loss: 0.0936 - acc: 0.9500\n",
      "Epoch 65/100\n",
      "120/120 [==============================] - 0s 235us/step - loss: 0.0842 - acc: 0.9833\n",
      "Epoch 66/100\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.0885 - acc: 0.9667\n",
      "Epoch 67/100\n",
      "120/120 [==============================] - 0s 181us/step - loss: 0.0816 - acc: 0.9667\n",
      "Epoch 68/100\n",
      "120/120 [==============================] - 0s 187us/step - loss: 0.0975 - acc: 0.9750\n",
      "Epoch 69/100\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.0946 - acc: 0.9583\n",
      "Epoch 70/100\n",
      "120/120 [==============================] - 0s 207us/step - loss: 0.0794 - acc: 0.9667\n",
      "Epoch 71/100\n",
      "120/120 [==============================] - 0s 274us/step - loss: 0.0883 - acc: 0.9583\n",
      "Epoch 72/100\n",
      "120/120 [==============================] - 0s 186us/step - loss: 0.0762 - acc: 0.9583\n",
      "Epoch 73/100\n",
      "120/120 [==============================] - 0s 218us/step - loss: 0.0528 - acc: 0.9917\n",
      "Epoch 74/100\n",
      "120/120 [==============================] - 0s 218us/step - loss: 0.1085 - acc: 0.9750\n",
      "Epoch 75/100\n",
      "120/120 [==============================] - 0s 251us/step - loss: 0.0661 - acc: 0.9833\n",
      "Epoch 76/100\n",
      "120/120 [==============================] - 0s 237us/step - loss: 0.0877 - acc: 0.9750\n",
      "Epoch 77/100\n",
      "120/120 [==============================] - 0s 190us/step - loss: 0.1012 - acc: 0.9667\n",
      "Epoch 78/100\n",
      "120/120 [==============================] - 0s 208us/step - loss: 0.0993 - acc: 0.9750\n",
      "Epoch 79/100\n",
      "120/120 [==============================] - 0s 234us/step - loss: 0.1062 - acc: 0.9583\n",
      "Epoch 80/100\n",
      "120/120 [==============================] - 0s 196us/step - loss: 0.0865 - acc: 0.9667\n",
      "Epoch 81/100\n",
      "120/120 [==============================] - 0s 239us/step - loss: 0.1089 - acc: 0.9583\n",
      "Epoch 82/100\n",
      "120/120 [==============================] - 0s 200us/step - loss: 0.0804 - acc: 0.9667\n",
      "Epoch 83/100\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.0835 - acc: 0.9667\n",
      "Epoch 84/100\n",
      "120/120 [==============================] - 0s 201us/step - loss: 0.1033 - acc: 0.9500\n",
      "Epoch 85/100\n",
      "120/120 [==============================] - 0s 218us/step - loss: 0.0620 - acc: 0.9667\n",
      "Epoch 86/100\n",
      "120/120 [==============================] - 0s 265us/step - loss: 0.0661 - acc: 0.9833\n",
      "Epoch 87/100\n",
      "120/120 [==============================] - 0s 253us/step - loss: 0.0858 - acc: 0.9667\n",
      "Epoch 88/100\n",
      "120/120 [==============================] - 0s 247us/step - loss: 0.0917 - acc: 0.9417\n",
      "Epoch 89/100\n",
      "120/120 [==============================] - 0s 224us/step - loss: 0.0659 - acc: 0.9750\n",
      "Epoch 90/100\n",
      "120/120 [==============================] - 0s 255us/step - loss: 0.0865 - acc: 0.9500\n",
      "Epoch 91/100\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.0788 - acc: 0.9667\n",
      "Epoch 92/100\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.0816 - acc: 0.9667\n",
      "Epoch 93/100\n",
      "120/120 [==============================] - 0s 220us/step - loss: 0.0922 - acc: 0.9667\n",
      "Epoch 94/100\n",
      "120/120 [==============================] - 0s 182us/step - loss: 0.0656 - acc: 0.9750\n",
      "Epoch 95/100\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.1112 - acc: 0.9667\n",
      "Epoch 96/100\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.0804 - acc: 0.9667\n",
      "Epoch 97/100\n",
      "120/120 [==============================] - 0s 176us/step - loss: 0.0542 - acc: 0.9750\n",
      "Epoch 98/100\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.0742 - acc: 0.9750\n",
      "Epoch 99/100\n",
      "120/120 [==============================] - 0s 174us/step - loss: 0.0793 - acc: 0.9750\n",
      "Epoch 100/100\n",
      "120/120 [==============================] - 0s 220us/step - loss: 0.0841 - acc: 0.9583\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "history = kr_model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display plots of loss & accuracies\n",
    "kru.show_plots(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating performance of Keras model:\n",
      "120/120 [==============================] - 0s 535us/step\n",
      " - Training dataset: loss = 0.0502, accuracy = 0.9750\n",
      "30/30 [==============================] - 0s 108us/step\n",
      " - Test dataset: loss = 0.0537, accuracy = 0.9667\n"
     ]
    }
   ],
   "source": [
    "# evaluate performance against train/cross-val & test data\n",
    "print('Evaluating performance of Keras model:')\n",
    "loss, acc = kr_model.evaluate(X_train, y_train, batch_size=batch_size)\n",
    "print(' - Training dataset: loss = {:.4f}, accuracy = {:.4f}'.format(loss, acc))\n",
    "\n",
    "loss, acc = kr_model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "print(' - Test dataset: loss = {:.4f}, accuracy = {:.4f}'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations for Keras Model:**\n",
    "* We get a `training accuracy = 98.3 %` and a `test accuracy = ~97%` with this model. **These are similar to Tensorflow model performance! **\n",
    "* Since the difference between training & cross-validation accuracy is not much, we can conclude the thet model is not overfitting the data.\n",
    "\n",
    "### Making Predictions with the Keras Model\n",
    "Next we will run predictions with our Tensorflow model & view some random results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 0, 1, 2, 0, 1, 0, 2, 1, 1, 2, 1, 1, 1, 2, 1, 0, 2, 1, 1, 1, 0, 0, 1, 0, 2, 2, 0])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the ground truth (reverse one-hot encode of test data)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "y_true[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 0, 1, 2, 0, 1, 0, 2, 1, 1, 2, 1, 2, 1, 2, 1, 0, 2, 1, 1, 1, 0, 0, 1, 0, 2, 2, 0])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = kr_model.predict(X_test)\n",
    "# reverse one-hot encode the predictions\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "y_pred[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no of mismatches\n",
    "(y_pred != y_true).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> -- END -- </center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
